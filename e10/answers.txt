1. 
(1): 16.682s
(2): 1m44.822s
(3): 22.592s
(4): 20.080s
2. Based on the above results, it looks like it takes spark most of the time to read the files
3. I used .cache() once right on the line where I defined "joined" variable
    joined = max_df.join(df, ['hour', 'count'], 'inner')\
                   .orderBy(['hour', 'name'], ascending=True)\
                   .drop('filename','language','bytes').cache()

	